## Formatting Example

To demonstrate format validation in practice, let's work with a real dataset: the Amazon Reviews 2023 dataset from McAuley Lab on Hugging Face. This dataset contains millions of product reviews with ratings, titles, and review textâ€”perfect for testing our product review analysis schema.

### The Dataset

The Amazon Reviews 2023 dataset includes reviews across multiple product categories. Each review contains:

- `rating`: A star rating from 1.0 to 5.0
- `title`: A short review headline
- `text`: The full review text
- Additional metadata (product IDs, timestamps, etc.)

We'll use this dataset to evaluate how well our LLM-based review analyzer produces valid, schema-conforming outputs. This mirrors a real-world scenario: you have existing data you want to process at scale, and you need to measure how reliably your LLM application handles it.

### Complete Evaluation Pipeline

The code in `code/format_validation_on_dataset.py` demonstrates a complete evaluation pipeline:

1. **Load the dataset**: Use Hugging Face's datasets library to load a sample of Amazon reviews
2. **Process each review**: Call the LLM to analyze sentiment and extract themes
3. **Validate outputs**: Check each response against our Pydantic schema
4. **Compute metrics**: Calculate conformance rates and identify failure patterns
5. **Report results**: Summarize which types of inputs cause validation failures

The implementation loads reviews from the Amazon Reviews 2023 dataset, processes them through the LLM, validates the outputs against our ReviewAnalysis schema, and computes conformance metrics. The key functions are:

- `analyze_review_from_dataset()`: Prompts the LLM to analyze a review
- `evaluate_on_dataset()`: Runs the full evaluation pipeline on N samples
- The main block saves results to JSON and provides interpretation

You can run the evaluation yourself:

```bash
python code/format_validation_on_dataset.py
```

This will process 50 reviews and report conformance metrics, showing exactly how reliable your format validation is on real data.

### What This Evaluation Reveals

Running this evaluation on real data tells you:

**Baseline conformance**: What percentage of outputs work without any fixes? This sets expectations for production reliability.

**Failure patterns**: Do failures cluster around certain types of reviews (very long, very short, specific topics)? This guides where to add handling logic.

**Cost vs quality tradeoffs**: Does truncating review text to save tokens hurt conformance? You can test different length limits.

**Prompt effectiveness**: Try variations of your prompt and measure which produces higher conformance rates.

**Model selection**: Test different models (gpt-4o-mini vs gpt-4o) to see if better models have higher conformance.

This type of systematic evaluation transforms anecdotal observations ("it usually works") into quantitative metrics ("92% conformance on 50 samples"). You can track these metrics over time, set acceptance thresholds for deployments, and make data-driven decisions about where to invest engineering effort.
