## Format Matching

The first step in evaluating structured outputs is establishing what "correct" means. Format matching is about defining schemas that specify the expected structure of your outputs, then systematically measuring how often your LLM application produces outputs that conform to those schemas.

### Why Format Failures Happen

Language models produce format failures for predictable reasons. They've been trained on text where JSON appears in many contexts: wrapped in markdown code blocks, embedded in explanatory prose, formatted with varying styles. When you prompt a model to "return JSON", it's drawing on patterns where JSON is often accompanied by helpful context.

Common formatting issues include:

- **Wrapper text**: "Here's the analysis you requested: {...}" or "The result is: {...}"
- **Markdown formatting**: Wrapping output in \`\`\`json ... \`\`\` code blocks
- **Structural variations**: Missing quotes around keys, trailing commas, inconsistent nesting
- **Schema deviations**: Including extra fields, missing required fields, or using unexpected field names
- **Type mismatches**: Returning a string where a number is expected, or a single value instead of a list

These aren't random errors. They reflect how JSON appears in the model's training data. Understanding this helps us design better evaluation strategies—we're not just checking for bugs, we're measuring how well we've aligned the model's natural output patterns with our application's requirements.

### Defining Schemas with Pydantic

Pydantic provides a powerful framework for defining and validating data structures. Unlike basic JSON schema validation, Pydantic models integrate type checking, custom validators, and clear error messages into a Pythonic interface. This makes them ideal for both runtime validation and evaluation measurement.

Let's work through a concrete example. Suppose you're building an application that analyzes product reviews and extracts structured insights. You want the model to return sentiment, key themes, and actionable recommendations. Here's how you'd define the expected schema:

```{.python include="code/product_review_schema.py" snippet="review_analysis"}
```

This schema encodes several layers of requirements:

**Type constraints**: `sentiment` must be one of four specific strings. `confidence` must be a float. `key_themes` must be a list of strings.

**Range constraints**: `confidence` must be between 0 and 1. `key_themes` must have 1-5 items.

**Optionality**: `product_issues` is optional (reviews might not mention problems).

**Custom validation**: Lists can't contain empty strings or duplicates.

These constraints capture both the structure your code requires and domain-specific requirements (like having at least one theme but no more than five).

### Measuring and Interpreting Conformance

With a schema defined, measuring conformance is straightforward: attempt to parse each LLM output as JSON, then validate it against your Pydantic schema. Track three key metrics:

**Parse rate**: Percentage of outputs that are valid JSON. Low parse rates (<70%) indicate the model isn't producing JSON at all—you need clearer formatting instructions in your prompt.

**Conformance rate**: Percentage that fully satisfy the schema. This is your primary quality metric. High conformance (>90%) means your prompt and schema are well-aligned. Low conformance despite high parse rates suggests the model produces valid JSON but with the wrong structure—clarify field names, types, and constraints in your prompt.

**Failure patterns**: When validation fails, record whether it's a JSON parsing error or a schema violation, and which fields are problematic. Systematic failures (e.g., 50% of outputs missing the same required field) indicate prompt engineering issues. Random failures suggest you need fallback handling for edge cases.

Implementing this is simple: call your LLM with diverse inputs, attempt to parse and validate each response, and compute aggregate statistics. The code in `format_evaluation.py` and `format_evaluation_example.py` demonstrates a complete evaluation pipeline. The key insight: conformance metrics are diagnostic tools that tell you where to focus your prompt engineering efforts.

### LLM Healing

When model responses deviate from the required format, a secondary "healing" pass can repair the structure without re-running the full task. We explore techniques for building healers that are transparent, measurable, and integrated into CI workflows.

Even with careful prompt engineering and schema validation, format failures will occur. The traditional approach is to build extensive error-handling logic: regex patterns for common malformations, type coercion for mismatched fields, default values for missing data. This logic becomes brittle and hard to maintain as schemas evolve.

LLM healing offers an alternative: when validation fails, use another LLM call to repair the malformed output rather than writing custom repair logic. The healer receives three inputs:

1. The expected schema specification
2. The actual malformed output that failed validation
3. The specific validation error encountered

The healer's task is straightforward: return a corrected version that conforms to the schema. This approach is particularly effective for structural errors—missing quotes, incorrect field names, wrong types—where the semantic content is correct but the formatting is off.

### A Practical Healing Example

Let's extend our product review example to include healing. When the model produces invalid JSON, we'll attempt to repair it automatically:

```{.python include="code/format_evaluation_with_healing.py" snippet="format_evaluation_healing"}
```

In this example, the original JSON is missing quotes around the `positive` value. Traditional JSON parsers would fail immediately. The healer receives the malformed output, sees the parse error, and returns a corrected version with proper quoting.

### When to Use Healing

Healing is most valuable when:

**The original generation was expensive**: If you spent significant tokens on a complex reasoning task and only the final formatting failed, healing can salvage that work without re-running the entire pipeline.

**The error is structural, not semantic**: Healing works well for missing quotes, wrong field names, or type mismatches. It's less effective when the model fundamentally misunderstood the task.

**You have diverse failure modes**: If failures happen in unpredictable ways, healing is more maintainable than writing custom repair logic for every possible error pattern.

Healing is less useful when:

**The initial generation is cheap**: For simple extraction tasks, retrying with an improved prompt may be more efficient than healing.

**Failures are systematic**: If 50% of outputs fail validation in the same way, fix your prompt rather than healing every failure.

**Semantic errors dominate**: If the model is producing valid JSON with wrong answers, healing won't help—you need better prompts or a different model.

### Implementation Considerations

**Model selection**: You can use a smaller, cheaper model for healing than for generation. Simple structural repairs don't require the same capability as the original task. However, for complex schema repairs, using the same model may be necessary.

**Temperature settings**: Use `temperature=0` or very low values for healing. You want deterministic repairs, not creative variations.

**Context inclusion**: For simple format repairs, the schema and error alone usually suffice. Including the original prompt context increases token usage without much benefit for structural fixes.

**Retry limits**: Most implementations limit healing to 1-2 attempts. Beyond that, you're likely dealing with a fundamental problem that healing can't fix.

The accompanying code in `code/format_evaluation_with_healing.py` demonstrates a complete implementation with configurable retry logic and metric tracking.
