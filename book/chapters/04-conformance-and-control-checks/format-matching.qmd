## Format Matching

The first step in evaluating structured outputs is establishing what "correct" means. Format matching is about defining schemas that specify the expected structure of your outputs, then systematically measuring how often your LLM application produces outputs that conform to those schemas.

### Why Format Failures Happen

Language models produce format failures for predictable reasons. They've been trained on text where JSON appears in many contexts: wrapped in markdown code blocks, embedded in explanatory prose, formatted with varying styles. When you prompt a model to "return JSON", it's drawing on patterns where JSON is often accompanied by helpful context.

Common formatting issues include:

- **Wrapper text**: "Here's the analysis you requested: {...}" or "The result is: {...}"
- **Markdown formatting**: Wrapping output in \`\`\`json ... \`\`\` code blocks
- **Structural variations**: Missing quotes around keys, trailing commas, inconsistent nesting
- **Schema deviations**: Including extra fields, missing required fields, or using unexpected field names
- **Type mismatches**: Returning a string where a number is expected, or a single value instead of a list

These aren't random errors. They reflect how JSON appears in the model's training data. Understanding this helps us design better evaluation strategies—we're not just checking for bugs, we're measuring how well we've aligned the model's natural output patterns with our application's requirements.

### Defining Schemas with Pydantic

Pydantic provides a powerful framework for defining and validating data structures. Unlike basic JSON schema validation, Pydantic models integrate type checking, custom validators, and clear error messages into a Pythonic interface. This makes them ideal for both runtime validation and evaluation measurement.

Let's work through a concrete example. Suppose you're building an application that analyzes product reviews and extracts structured insights. You want the model to return sentiment, key themes, and actionable recommendations. Here's how you'd define the expected schema:

```{.python include="code/product_review_schema.py" snippet="review_analysis"}
```

This schema encodes several layers of requirements:

**Type constraints**: `sentiment` must be one of four specific strings. `confidence` must be a float. `key_themes` must be a list of strings.

**Range constraints**: `confidence` must be between 0 and 1. `key_themes` must have 1-5 items.

**Optionality**: `product_issues` is optional (reviews might not mention problems).

**Custom validation**: Lists can't contain empty strings or duplicates.

These constraints capture both the structure your code requires and domain-specific requirements (like having at least one theme but no more than five).

### Measuring Conformance

With a schema defined, you can systematically measure how often your LLM produces valid outputs. Let's implement a complete evaluation pipeline:

```{.python include="code/format_evaluation.py" snippet="format_evaluation"}
```

Now let's see this in action with a real example using the OpenAI API:

```{.python include="code/format_evaluation_example.py" snippet="format_evaluation_example"}
```

This evaluation pipeline gives you concrete numbers: what percentage of outputs are usable? When failures occur, are they JSON parsing issues or schema violations? This data helps you diagnose problems and track improvements.

### Interpreting Conformance Metrics

Conformance metrics tell different stories:

**High conformance (>90%)**: Your prompt and schema are well-aligned. The model consistently produces usable outputs. Focus on evaluating semantic correctness rather than structural issues.

**Low parse rate (<70%)**: The model isn't producing valid JSON. This suggests prompt engineering issues—you may need to be more explicit about requiring JSON output, provide examples, or use system messages to enforce formatting.

**High parse rate but low conformance**: The model produces valid JSON but with the wrong structure. This often indicates schema misalignment—the model interprets your requirements differently than intended. Review your prompt to ensure field names, types, and constraints are clearly specified.

**Inconsistent conformance**: Some inputs produce valid outputs, others don't. Look for patterns in failures—do they occur with longer inputs, specific content types, or edge cases? This helps you understand when your application needs fallback handling.

The key insight: conformance rate is not just a quality metric, it's a diagnostic tool. It tells you where your prompt engineering efforts should focus and whether schema changes improve or degrade reliability.

### LLM Healing

When model responses deviate from the required format, a secondary "healing" pass can repair the structure without re-running the full task. We explore techniques for building healers that are transparent, measurable, and integrated into CI workflows.

Even with careful prompt engineering and schema validation, format failures will occur. The traditional approach is to build extensive error-handling logic: regex patterns for common malformations, type coercion for mismatched fields, default values for missing data. This logic becomes brittle and hard to maintain as schemas evolve.

LLM healing offers an alternative: when validation fails, use another LLM call to repair the malformed output rather than writing custom repair logic. The healer receives three inputs:

1. The expected schema specification
2. The actual malformed output that failed validation
3. The specific validation error encountered

The healer's task is straightforward: return a corrected version that conforms to the schema. This approach is particularly effective for structural errors—missing quotes, incorrect field names, wrong types—where the semantic content is correct but the formatting is off.

### A Practical Healing Example

Let's extend our product review example to include healing. When the model produces invalid JSON, we'll attempt to repair it automatically:

```{.python include="code/format_evaluation_with_healing.py" snippet="format_evaluation_healing"}
```

In this example, the original JSON is missing quotes around the `positive` value. Traditional JSON parsers would fail immediately. The healer receives the malformed output, sees the parse error, and returns a corrected version with proper quoting.

### When to Use Healing

Healing is most valuable when:

**The original generation was expensive**: If you spent significant tokens on a complex reasoning task and only the final formatting failed, healing can salvage that work without re-running the entire pipeline.

**The error is structural, not semantic**: Healing works well for missing quotes, wrong field names, or type mismatches. It's less effective when the model fundamentally misunderstood the task.

**You have diverse failure modes**: If failures happen in unpredictable ways, healing is more maintainable than writing custom repair logic for every possible error pattern.

Healing is less useful when:

**The initial generation is cheap**: For simple extraction tasks, retrying with an improved prompt may be more efficient than healing.

**Failures are systematic**: If 50% of outputs fail validation in the same way, fix your prompt rather than healing every failure.

**Semantic errors dominate**: If the model is producing valid JSON with wrong answers, healing won't help—you need better prompts or a different model.

### Implementation Considerations

**Model selection**: You can use a smaller, cheaper model for healing than for generation. Simple structural repairs don't require the same capability as the original task. However, for complex schema repairs, using the same model may be necessary.

**Temperature settings**: Use `temperature=0` or very low values for healing. You want deterministic repairs, not creative variations.

**Context inclusion**: For simple format repairs, the schema and error alone usually suffice. Including the original prompt context increases token usage without much benefit for structural fixes.

**Retry limits**: Most implementations limit healing to 1-2 attempts. Beyond that, you're likely dealing with a fundamental problem that healing can't fix.

The accompanying code in `code/format_evaluation_with_healing.py` demonstrates a complete implementation with configurable retry logic and metric tracking.
