## Routing and Classification

Classification and routing tasks form the backbone of many LLM applications. Unlike open-ended generation, these tasks have discrete outcomes: assigning a label, choosing a category, or directing a request to the appropriate handler. The evaluation question is clear: did the model make the right choice?

These tasks appear throughout production systems:

**Customer support routing**: Incoming tickets need to be directed to the right team (billing, technical, sales). Route incorrectly and customers wait longer while tickets are manually reassigned.

**Content moderation**: User-generated content must be classified for policy violations (spam, harassment, inappropriate material). False negatives allow harmful content through; false positives frustrate legitimate users.

**Priority assignment**: Not all tasks are equally urgent. Systems need to identify which customer issues require immediate attention versus which can be handled in the normal queue.

**Intent recognition**: Conversational interfaces must understand what users want (search for information, make a purchase, file a complaint) to provide appropriate responses.

**Document triage**: Organizations process thousands of documents daily. Automatic classification by type, topic, or required action enables efficient workflows.

What makes these tasks particularly important for evaluation is that they're **measurable**. Unlike assessing whether generated text is "good," classification accuracy can be computed precisely. You can track how often the model makes correct decisions, what types of errors occur, and whether performance is improving or degrading over time.

### How LLMs Perform Classification

Language models weren't originally designed for classification—they generate text by predicting the next token. But they've proven remarkably effective at classification tasks through a simple pattern: frame classification as text generation with constrained outputs.

Consider ticket routing. Instead of training a specialized classifier, you prompt the model:

```
Classify this support ticket into one of these categories:
- billing
- technical
- sales

Ticket: "I was charged twice for my subscription this month"
Category:
```

The model generates text ("billing") which you then treat as a classification decision. This approach has several advantages:

**No training required**: You can add new categories or change the taxonomy without retraining. Just update the prompt.

**Contextual reasoning**: The model can consider nuances that rule-based systems miss. A ticket about "canceling because the product doesn't work" might be both technical and billing—the model can make judgment calls.

**Explanation capability**: You can ask the model to explain its classification, providing transparency for debugging and user trust.

**Few-shot learning**: Including 2-3 examples in the prompt often dramatically improves accuracy for ambiguous cases.

However, this flexibility comes with challenges. The model might generate text that doesn't match any category. It might be inconsistent across similar inputs. These are the kinds of failures we need to measure and track.

### A Classification Example

Let's build a simple email priority classifier that demonstrates evaluation patterns for classification tasks. First, we define the schema for our classification result:

```{.python include="code/email_classifier.py" snippet="schema"}
```

This Pydantic model ensures our classification always returns a valid priority level and includes reasoning for transparency.

Now we implement the classifier function that calls the LLM:

```{.python include="code/email_classifier.py" snippet="classify"}
```

This example demonstrates several important patterns:

**Structured output**: The classification returns not just a label but also reasoning, enabling richer evaluation and debugging.

**Clear category definitions**: The prompt explicitly defines what each priority level means, reducing ambiguity.

**Explicit validation**: We compare predictions against expected labels to compute accuracy.

### Evaluating Classification Performance

For classification tasks, we can apply standard metrics from machine learning:

**Accuracy**: The percentage of correct predictions. Simple and interpretable, but can be misleading with imbalanced classes.

**Confusion matrix**: Shows which categories are confused with each other. If "normal" emails are frequently misclassified as "urgent," you'll see it immediately.

**Precision and recall**: For each category, precision asks "of the emails we labeled as urgent, how many actually were?" while recall asks "of the actually urgent emails, how many did we catch?"

These metrics tell different stories about classifier performance. High accuracy with low recall for "urgent" means you're missing critical emails. High precision but low accuracy means you're being too conservative. The confusion matrix shows you exactly which categories need better distinction in your prompts.

The code in `code/classification_metrics.py` provides implementations of these standard metrics including confusion matrices and per-class precision/recall calculations.

### Iterating on Classification Prompts

The beauty of LLM-based classification is that you can improve performance by refining prompts rather than retraining models. Common improvements:

**Add few-shot examples**: Include 2-3 examples of each category, especially for ambiguous cases.

**Clarify category boundaries**: Make explicit distinctions between similar categories ("urgent means active outage, normal means degraded performance").

**Request chain-of-thought**: Ask the model to think through the classification step by step before deciding.

**Provide context**: If routing decisions depend on customer tier or time of day, include that information.

Each prompt change can be evaluated using the metrics above. Did accuracy improve? Did the confusion matrix change? This tight feedback loop makes prompt engineering for classification tasks particularly tractable.

The example code in `code/email_classifier.py` and `code/classification_metrics.py` provides a complete framework for evaluating classification tasks with metrics tracking.
